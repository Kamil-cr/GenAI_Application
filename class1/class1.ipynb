{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ : bool  = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "client : OpenAI = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-8phOiuVozJGJRl4PskkbIM0L1xDQM',\n",
       " 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='11 + 1 = 12', role='assistant', function_call=None, tool_calls=None))],\n",
       " 'created': 1707332240,\n",
       " 'model': 'gpt-3.5-turbo-0613',\n",
       " 'object': 'chat.completion',\n",
       " 'system_fingerprint': None,\n",
       " 'usage': CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 + 1 = 12\n"
     ]
    }
   ],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "\n",
    "def chat_completion(prompt: str) -> object:\n",
    "    response : ChatCompletion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    display(dict(response))\n",
    "    # print(response)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(chat_completion(\"What is 11+1?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the land of loops and functions we dwell,\n",
      "Where patterns of logic and sequences swell.\n",
      "But a concept more mystical, beyond comprehension,\n",
      "Is the beauty of recursive intervention.\n",
      "\n",
      "Like a mirror reflecting its own\n"
     ]
    }
   ],
   "source": [
    "def chat_completion(prompt: str) -> str:\n",
    "    completion : ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=40,\n",
    "    )\n",
    "    # print(response)\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "print(chat_completion(\"Compose a poem that explains the concept of recursion in programming.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This\n",
      " is\n",
      " a\n",
      " test\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    messages = [{\"role\": \"user\", \"content\": \"say this is a test\"}],\n",
    "    stream = True,\n",
    "    \n",
    ")\n",
    "\n",
    "for part in stream:\n",
    "    print(part.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"months_with_30_days\": [\n",
      "    \"April\",\n",
      "    \"June\",\n",
      "    \"September\",\n",
      "    \"November\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\": \"List of months that have 30 days.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GenAI_Application\\class1\\class1.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GenAI_Application/class1/class1.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GenAI_Application/class1/class1.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Parse response\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GenAI_Application/class1/class1.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m obj: Dict[\u001b[39mstr\u001b[39m, \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GenAI_Application/class1/class1.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# The result is python Dictionary:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GenAI_Application/class1/class1.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(obj[\u001b[39m'\u001b[39m\u001b[39mmonths_with_30_days\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "# Parse response\n",
    "obj: Dict[str, list[str]] = json.loads(response.choices[0].message.content)\n",
    "\n",
    "# The result is python Dictionary:\n",
    "print(obj['months_with_30_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str, unit: str= \"fahrenheit\")-> str: \n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"san francisco\", \"temperature\": \"50\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "import json\n",
    "\n",
    "def run_conversation(main_request: str) -> object:\n",
    "    messages = [{\"role\": \"user\", \"content\": main_request}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The City and state, e.g. San Francisco, CA\"\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First Request\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "    )\n",
    "    response_message: ChatCompletion = response.choices[0].message\n",
    "    display(\"* First Response: \", dict(response_message))\n",
    "\n",
    "    tool_calls = response_message.tool_calls\n",
    "    display(\"* First Response Tool Calls: \", list(tool_calls))\n",
    "\n",
    "    # Step 2: check if the model wanted to call a function \n",
    "    if tool_calls:\n",
    "        # Step 3: Call the function \n",
    "        # note: The JSON response may not always be valid; be sure to handle errors  \n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather\n",
    "        } # only one function in this example, but you can have multiple\n",
    "\n",
    "        messages.append(response_message) # extend conversation with assisstant's reply\n",
    "\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls :\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )# extend conversation with function response\n",
    "        display(\"* Second Request Messages: \", list(messages))   \n",
    "        second_response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        ) # get a new response \n",
    "        print(\"* Second Response: \", dict(second_response))\n",
    "        return second_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* First Response: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'content': None,\n",
       " 'role': 'assistant',\n",
       " 'function_call': None,\n",
       " 'tool_calls': [ChatCompletionMessageToolCall(id='call_L1QuQWD9SQpDWKMvjhllzq0j', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       "  ChatCompletionMessageToolCall(id='call_9nD0ketC8vmLJInuLUHCKB8a', function=Function(arguments='{\"location\": \"Tokyo, Japan\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       "  ChatCompletionMessageToolCall(id='call_laksRa2eRU7OncB3WJhyOKrp', function=Function(arguments='{\"location\": \"Paris, France\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* First Response Tool Calls: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_L1QuQWD9SQpDWKMvjhllzq0j', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_9nD0ketC8vmLJInuLUHCKB8a', function=Function(arguments='{\"location\": \"Tokyo, Japan\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'),\n",
       " ChatCompletionMessageToolCall(id='call_laksRa2eRU7OncB3WJhyOKrp', function=Function(arguments='{\"location\": \"Paris, France\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* Second Request Messages: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'What is the weather like in San Francisco, Tokyo, and Paris?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_L1QuQWD9SQpDWKMvjhllzq0j', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_9nD0ketC8vmLJInuLUHCKB8a', function=Function(arguments='{\"location\": \"Tokyo, Japan\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_laksRa2eRU7OncB3WJhyOKrp', function=Function(arguments='{\"location\": \"Paris, France\", \"unit\": \"celsius\"}', name='get_current_weather'), type='function')]),\n",
       " {'tool_call_id': 'call_L1QuQWD9SQpDWKMvjhllzq0j',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"san francisco\", \"temperature\": \"50\", \"unit\": \"fahrenheit\"}'},\n",
       " {'tool_call_id': 'call_9nD0ketC8vmLJInuLUHCKB8a',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"}'},\n",
       " {'tool_call_id': 'call_laksRa2eRU7OncB3WJhyOKrp',\n",
       "  'role': 'tool',\n",
       "  'name': 'get_current_weather',\n",
       "  'content': '{\"location\": \"Paris\", \"temperature\": \"10\", \"unit\": \"celsius\"}'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Second Response:  {'id': 'chatcmpl-8pgAZLIO6GpRLz2wULRp5FICwVkYB', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Currently, the weather in San Francisco is 50°F, in Tokyo it is 10°C, and in Paris it is also 10°C.', role='assistant', function_call=None, tool_calls=None))], 'created': 1707327519, 'model': 'gpt-3.5-turbo-1106', 'object': 'chat.completion', 'system_fingerprint': 'fp_04f9a1eebf', 'usage': CompletionUsage(completion_tokens=30, prompt_tokens=176, total_tokens=206)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Currently, the weather in San Francisco is 50°F, in Tokyo it is 10°C, and in Paris it is also 10°C.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conversation(\"What is the weather like in San Francisco, Tokyo, and Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_function(order_name: str) -> str:\n",
    "    if order_name == \"pizza\":\n",
    "        return json.dumps({\"order\": \"pizza\", \"status\": \"confirmed\"})\n",
    "    elif order_name == \"burger\":\n",
    "        return json.dumps({\"order\": \"burger\", \"status\": \"confirmed\"})\n",
    "    else:\n",
    "        return json.dumps({\"order\": order_name, \"status\": \"unknown\"})\n",
    "    \n",
    "def run_conversation(main_request: str) -> object:\n",
    "    messages: list = [{\"role\": \"user\", \"content\": main_request}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"order_function\",\n",
    "                \"description\": \"Place an order for food\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"order_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The name of the food to order, e.g. pizza, burger\"\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"order_name\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # First Request\n",
    "    response: ChatCompletion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    response_message: ChatCompletion = response.choices[0].message\n",
    "    display(\"* First Response: \", dict(response_message))\n",
    "\n",
    "    tool_calls = response_message.tool_calls\n",
    "    display(\"* First Response Tool Calls: \", list(tool_calls))\n",
    "\n",
    "    # Step 2: check if the model wanted to call a function \n",
    "    if tool_calls:\n",
    "        # Step 3: Call the function \n",
    "        # note: The JSON response may not always be valid; be sure to handle errors  \n",
    "        available_functions = {\n",
    "            \"order_function\": order_function\n",
    "        } # only one function in this example, but you can have multiple\n",
    "\n",
    "        messages.append(response_message) # extend conversation with assisstant's reply\n",
    "\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls :\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                order_name=function_args.get(\"order_name\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )# extend conversation with function response\n",
    "        display(\"* Second Request Messages: \", list(messages))   \n",
    "        second_response: ChatCompletion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        ) # get a new response\n",
    "        print(\"* Second Response: \", dict(second_response))\n",
    "        return second_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* First Response: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'content': None,\n",
       " 'role': 'assistant',\n",
       " 'function_call': None,\n",
       " 'tool_calls': [ChatCompletionMessageToolCall(id='call_xASOClgPMlKFIGaMO3hr7zfv', function=Function(arguments='{\"order_name\": \"pizza\"}', name='order_function'), type='function')]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* First Response Tool Calls: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_xASOClgPMlKFIGaMO3hr7zfv', function=Function(arguments='{\"order_name\": \"pizza\"}', name='order_function'), type='function')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'* Second Request Messages: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Pizza order karna ha'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_xASOClgPMlKFIGaMO3hr7zfv', function=Function(arguments='{\"order_name\": \"pizza\"}', name='order_function'), type='function')]),\n",
       " {'tool_call_id': 'call_xASOClgPMlKFIGaMO3hr7zfv',\n",
       "  'role': 'tool',\n",
       "  'name': 'order_function',\n",
       "  'content': '{\"order\": \"pizza\", \"status\": \"confirmed\"}'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Second Response:  {'id': 'chatcmpl-8phAq9xeGbZDcEkyt1dFVeJZDDLfU', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Pizza order successfully placed!', role='assistant', function_call=None, tool_calls=None))], 'created': 1707331380, 'model': 'gpt-3.5-turbo-1106', 'object': 'chat.completion', 'system_fingerprint': 'fp_04f9a1eebf', 'usage': CompletionUsage(completion_tokens=5, prompt_tokens=47, total_tokens=52)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Pizza order successfully placed!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_conversation(\"Pizza order karna ha\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
